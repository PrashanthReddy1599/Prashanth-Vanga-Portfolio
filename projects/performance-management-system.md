# Performance Management System

## üìÑ Project Overview

Academic capstone project developed during Master's program at University of Texas at Arlington (Jan 2024 - May 2024). Built a large-scale data processing system for network performance monitoring, transforming raw XML data into analytics-ready formats and designing custom KPIs for anomaly detection.

## üéØ Project Objectives

- Transform 500K+ daily XML records into structured CSV format
- Build scalable data pipelines using Google Cloud Platform services
- Design 18 custom KPIs for network performance monitoring
- Implement anomaly detection capabilities with 20% improvement
- Optimize query performance through database tuning and indexing

## üõ†Ô∏è Technologies & Tools

### Cloud Platform & Services
- **Google Cloud Platform (GCP)** - Primary cloud infrastructure
- **Google Cloud Dataflow** - Serverless data processing service
- **Apache Beam** - Unified programming model for batch and streaming data
- **BigQuery** - Serverless, highly scalable data warehouse
- **Cloud SQL** - Fully managed relational database service (MySQL/PostgreSQL)
- **Google Cloud Storage (GCS)** - Object storage for data lake

### Data Processing & Transformation
- **Python** - Primary programming language for data processing
- **Apache Beam SDK** - Data pipeline development framework
- **Pandas** - Data manipulation and analysis
- **SQL** - Query optimization and database design

### Monitoring & Analytics
- **Dimensional Modeling** - Star schema design for analytics
- **Data Validation Frameworks** - Quality assurance at every stage
- **Custom KPI Dashboards** - Performance visualization and alerting

## üìä System Architecture

```
[Network Monitoring Systems]
         ‚Üì
    [XML Data Feed]
    (500K records/day)
         ‚Üì
 [Google Cloud Storage]
    (Raw Data Lake)
         ‚Üì
 [Cloud Dataflow Pipeline]
  (Apache Beam Processing)
         ‚Üì
  [Data Transformation]
    - XML Parsing
    - Schema Validation
    - Format Conversion
    - Business Logic
         ‚Üì
   [CSV Output Files]
         ‚Üì
    [Cloud SQL + BigQuery]
    (Analytics Layer)
         ‚Üì
  [KPI Dashboards]
  (Anomaly Detection)
```

## üöÄ Key Features & Implementation

### 1. Large-Scale Data Processing Pipeline

**Challenge:** Process 500K+ XML records daily with varying schemas and nested structures

**Solution:**
- Built Apache Beam pipeline with custom DoFn transformations
- Implemented parallel processing using Cloud Dataflow workers
- Applied windowing for micro-batch processing
- Created dead letter queues for error handling

**Results:**
- Successfully processed 500K+ records daily
- 35% improvement in data accuracy through validation
- 95% reduction in processing time compared to sequential approach

### 2. XML to CSV Transformation

**Challenge:** Complex XML structures with nested elements and varying schemas

**Implementation:**
```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import xml.etree.ElementTree as ET

class XMLToCSVTransform(beam.DoFn):
    def process(self, element):
        try:
            # Parse XML
            root = ET.fromstring(element)
            
            # Extract fields
            record = {
                'timestamp': root.find('timestamp').text,
                'metric_name': root.find('metric/name').text,
                'metric_value': root.find('metric/value').text,
                'device_id': root.find('device/id').text,
                # ... additional fields
            }
            
            # Convert to CSV format
            csv_line = ','.join(str(v) for v in record.values())
            yield csv_line
            
        except Exception as e:
            # Send to dead letter queue
            yield beam.pvalue.TaggedOutput('errors', element)

# Pipeline definition
with beam.Pipeline(options=PipelineOptions()) as pipeline:
    (
        pipeline
        | 'Read XML' >> beam.io.ReadFromText('gs://bucket/xml-files/*.xml')
        | 'Transform' >> beam.ParDo(XMLToCSVTransform())
        | 'Write CSV' >> beam.io.WriteToText('gs://bucket/csv-output/')
    )
```

**Results:**
- Successfully transformed 500K+ daily records
- 35% improvement in data accuracy
- Automated error handling and validation

### 3. Custom KPI Design & Implementation

**Designed 18 KPIs across multiple dimensions:**

#### Network Performance KPIs
1. **Latency Metrics**
   - Average latency per device
   - 95th percentile latency
   - Latency variance over time

2. **Throughput Metrics**
   - Data transfer rate
   - Packet loss percentage
   - Bandwidth utilization

3. **Availability Metrics**
   - System uptime percentage
   - Mean time between failures (MTBF)
   - Mean time to recovery (MTTR)

4. **Quality Metrics**
   - Error rate per endpoint
   - Success rate of transactions
   - Data quality score

#### Dimensional Model Implementation

```sql
-- Dimension Tables
CREATE TABLE dim_device (
    device_key INT PRIMARY KEY,
    device_id VARCHAR(50),
    device_type VARCHAR(50),
    location VARCHAR(100),
    manufacturer VARCHAR(100)
);

CREATE TABLE dim_time (
    time_key INT PRIMARY KEY,
    date DATE,
    hour INT,
    day_of_week VARCHAR(20),
    month INT,
    quarter INT,
    year INT
);

CREATE TABLE dim_metric (
    metric_key INT PRIMARY KEY,
    metric_name VARCHAR(100),
    metric_category VARCHAR(50),
    unit_of_measure VARCHAR(20)
);

-- Fact Table
CREATE TABLE fact_network_performance (
    performance_key BIGINT PRIMARY KEY,
    device_key INT,
    time_key INT,
    metric_key INT,
    metric_value DECIMAL(18,4),
    quality_score INT,
    anomaly_flag BOOLEAN,
    FOREIGN KEY (device_key) REFERENCES dim_device(device_key),
    FOREIGN KEY (time_key) REFERENCES dim_time(time_key),
    FOREIGN KEY (metric_key) REFERENCES dim_metric(metric_key)
);

-- Partitioning for performance
ALTER TABLE fact_network_performance
PARTITION BY RANGE (time_key);
```

### 4. Anomaly Detection System

**Approach:**
- Statistical analysis using rolling averages and standard deviations
- Threshold-based alerting for critical KPIs
- Machine learning models for pattern recognition

**Implementation:**
```sql
-- Anomaly detection query
WITH metrics_stats AS (
    SELECT 
        device_key,
        metric_key,
        AVG(metric_value) as avg_value,
        STDDEV(metric_value) as std_dev
    FROM fact_network_performance
    WHERE time_key >= CURRENT_DATE - 30
    GROUP BY device_key, metric_key
),
current_metrics AS (
    SELECT *
    FROM fact_network_performance
    WHERE time_key = CURRENT_DATE
)
SELECT 
    cm.device_key,
    cm.metric_key,
    cm.metric_value,
    ms.avg_value,
    ms.std_dev,
    CASE 
        WHEN ABS(cm.metric_value - ms.avg_value) > 2 * ms.std_dev 
        THEN TRUE 
        ELSE FALSE 
    END as is_anomaly
FROM current_metrics cm
JOIN metrics_stats ms 
    ON cm.device_key = ms.device_key 
    AND cm.metric_key = ms.metric_key
WHERE ABS(cm.metric_value - ms.avg_value) > 2 * ms.std_dev;
```

**Results:**
- 20% improvement in anomaly detection accuracy
- Reduced false positives through statistical modeling
- Real-time alerting for critical performance issues

### 5. Cloud SQL Optimization

**Challenge:** Query latency for complex analytics queries

**Optimization Strategies:**
1. **Indexing**
   - Created composite indexes on frequently queried columns
   - Optimized index selection based on query patterns
   
2. **Query Optimization**
   - Rewrote subqueries as JOINs
   - Implemented materialized views for complex aggregations
   - Added query result caching

3. **Database Tuning**
   - Optimized connection pooling
   - Configured appropriate buffer pool sizes
   - Enabled query performance insights

**Results:**
- 15% reduction in query latency
- 40% improvement in concurrent query throughput
- Better resource utilization

## üìä Project Results & Impact

### Performance Metrics
- **Processing Speed:** 500K+ records processed daily
- **Data Accuracy:** 35% improvement through validation frameworks
- **Query Performance:** 15% reduction in latency
- **Anomaly Detection:** 20% improvement in detection accuracy

### Technical Achievements
- Successfully implemented serverless architecture on GCP
- Built scalable data pipelines using Apache Beam/Dataflow
- Designed dimensional model with star schema
- Created comprehensive KPI framework (18 metrics)
- Implemented automated data quality checks

### Learning Outcomes
- Gained hands-on experience with Google Cloud Platform
- Mastered Apache Beam for distributed data processing
- Applied dimensional modeling principles in real-world scenario
- Developed skills in data pipeline optimization
- Enhanced understanding of anomaly detection techniques

## üìù Implementation Timeline

### Phase 1: Requirements & Design (2 weeks)
- Gathered requirements for network performance monitoring
- Designed system architecture and data flow
- Created dimensional model and KPI framework
- Selected appropriate GCP services

### Phase 2: Pipeline Development (4 weeks)
- Built Apache Beam pipeline for XML parsing
- Implemented data transformation logic
- Created validation frameworks
- Set up Cloud Dataflow jobs

### Phase 3: Data Warehouse Design (2 weeks)
- Designed star schema in Cloud SQL and BigQuery
- Created dimension and fact tables
- Implemented partitioning and indexing strategies
- Built ETL processes for data loading

### Phase 4: KPI Development (3 weeks)
- Designed 18 custom KPIs
- Implemented SQL queries for metric calculation
- Built anomaly detection algorithms
- Created dashboards and visualizations

### Phase 5: Optimization & Testing (3 weeks)
- Performance tuning of queries and pipelines
- Load testing with production-scale data
- Bug fixes and refinements
- Documentation and presentation preparation

## üìù Lessons Learned

### Technical Insights
1. **Serverless architecture** (Dataflow, BigQuery) greatly simplified operations
2. **Apache Beam** provided excellent abstraction for data processing
3. **Dimensional modeling** was crucial for efficient analytics queries
4. **Automated validation** caught data quality issues early
5. **Query optimization** had the biggest impact on user experience

### Best Practices Applied
- Design for scalability from the beginning
- Implement comprehensive error handling and logging
- Use managed services to reduce operational overhead
- Apply dimensional modeling principles for analytics
- Monitor and optimize performance continuously

### Challenges Overcome
1. **Complex XML structures** - Solved with robust parsing and validation
2. **Data quality issues** - Implemented multi-layer validation framework
3. **Query performance** - Applied indexing and optimization techniques
4. **Scalability concerns** - Leveraged serverless architecture

## üîÆ Future Enhancements

If continuing this project, potential improvements would include:

- Real-time streaming with Pub/Sub and Dataflow streaming
- Machine learning models for predictive anomaly detection
- Advanced visualization with Looker or Data Studio
- Multi-cloud deployment for disaster recovery
- Integration with monitoring and alerting systems (PagerDuty, Slack)

## üéØ Skills Demonstrated

### Technical Skills
- Google Cloud Platform (Dataflow, BigQuery, Cloud SQL, GCS)
- Apache Beam for distributed data processing
- Python for data engineering and transformation
- SQL for data modeling and analytics
- Dimensional modeling and data warehouse design

### Soft Skills
- Project planning and execution
- Technical documentation and presentation
- Problem-solving and critical thinking
- Independent learning and research
- Stakeholder communication

---

**Academic Context:** This project was completed as part of the M.S. in Business Analytics program at University of Texas at Arlington (January 2024 - May 2024). All data used in this project was anonymized and generated for educational purposes.

**Note:** This project description uses anonymized data and does not contain any proprietary or sensitive information. All metrics are representative of real-world performance but have been generalized for portfolio purposes.
